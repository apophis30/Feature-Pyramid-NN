{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmpqVyH/A0Fh2QHujKkDub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apophis30/Feature-Pyramid-NN/blob/main/FeaturePyramidNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Union, Tuple, List, NamedTuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch import nn, Tensor\n",
        "import torchvision.models\n",
        "\n",
        "class ResNet50:\n",
        "    class ConvLayers(NamedTuple):\n",
        "        conv1: nn.Module\n",
        "        conv2: nn.Module\n",
        "        conv3: nn.Module\n",
        "        conv4: nn.Module\n",
        "        conv5: nn.Module\n",
        "\n",
        "    class LateralLayers(NamedTuple):\n",
        "        lateral_c2: nn.Module\n",
        "        lateral_c3: nn.Module\n",
        "        lateral_c4: nn.Module\n",
        "        lateral_c5: nn.Module\n",
        "\n",
        "    class DealiasingLayers(NamedTuple):\n",
        "        dealiasing_p2: nn.Module\n",
        "        dealiasing_p3: nn.Module\n",
        "        dealiasing_p4: nn.Module\n",
        "\n",
        "    def __init__(self, pretrained: bool):\n",
        "        self._pretrained = pretrained\n",
        "\n",
        "    def features(self) -> Tuple[ConvLayers, LateralLayers, DealiasingLayers, int]:\n",
        "        resnet50 = torchvision.models.resnet50(pretrained=self._pretrained)\n",
        "        children = list(resnet50.children())\n",
        "\n",
        "        conv1 = nn.Sequential(*children[:4])\n",
        "        conv2 = children[4]\n",
        "        conv3 = children[5]\n",
        "        conv4 = children[6]\n",
        "        conv5 = children[7]\n",
        "\n",
        "        num_features_out = 256\n",
        "\n",
        "        lateral_c2 = nn.Conv2d(in_channels=256, out_channels=num_features_out, kernel_size=1)\n",
        "        lateral_c3 = nn.Conv2d(in_channels=512, out_channels=num_features_out, kernel_size=1)\n",
        "        lateral_c4 = nn.Conv2d(in_channels=1024, out_channels=num_features_out, kernel_size=1)\n",
        "        lateral_c5 = nn.Conv2d(in_channels=2048, out_channels=num_features_out, kernel_size=1)\n",
        "\n",
        "        dealiasing_p2 = nn.Conv2d(in_channels=num_features_out, out_channels=num_features_out, kernel_size=3, padding=1)\n",
        "        dealiasing_p3 = nn.Conv2d(in_channels=num_features_out, out_channels=num_features_out, kernel_size=3, padding=1)\n",
        "        dealiasing_p4 = nn.Conv2d(in_channels=num_features_out, out_channels=num_features_out, kernel_size=3, padding=1)\n",
        "\n",
        "        for module in [conv1, conv2]:\n",
        "            for parameter in module.parameters():\n",
        "                parameter.requires_grad = False\n",
        "\n",
        "        conv_layers = self.ConvLayers(conv1, conv2, conv3, conv4, conv5)\n",
        "        lateral_layers = self.LateralLayers(lateral_c2, lateral_c3, lateral_c4, lateral_c5)\n",
        "        dealiasing_layers = self.DealiasingLayers(dealiasing_p2, dealiasing_p3, dealiasing_p4)\n",
        "\n",
        "        return conv_layers, lateral_layers, dealiasing_layers, num_features_out"
      ],
      "metadata": {
        "id": "bMSbgio87IfG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    class ForwardInput(object):\n",
        "        class Train(NamedTuple):\n",
        "            image: Tensor\n",
        "\n",
        "\n",
        "        class Eval(NamedTuple):\n",
        "            image: Tensor\n",
        "\n",
        "    class ForwardOutput(object):\n",
        "        class Train(NamedTuple):\n",
        "            output: Tensor\n",
        "\n",
        "        class Eval(NamedTuple):\n",
        "            output: Tensor\n",
        "\n",
        "    def __init__(self, backbone, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        resnet = ResNet50(pretrained=True)\n",
        "        conv_layers, lateral_layers, dealiasing_layers, num_features_out = resnet.features()\n",
        "        self.conv1, self.conv2, self.conv3, self.conv4, self.conv5 = conv_layers\n",
        "        self.lateral_c2, self.lateral_c3, self.lateral_c4, self.lateral_c5 = lateral_layers\n",
        "        self.dealiasing_p2, self.dealiasing_p3, self.dealiasing_p4 = dealiasing_layers\n",
        "\n",
        "        self._bn_modules = [it for it in self.conv1.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.conv2.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.conv3.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.conv4.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.conv5.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.lateral_c2.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.lateral_c3.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.lateral_c4.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.lateral_c5.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.dealiasing_p2.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.dealiasing_p3.modules() if isinstance(it, nn.BatchNorm2d)] + \\\n",
        "                           [it for it in self.dealiasing_p4.modules() if isinstance(it, nn.BatchNorm2d)]\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, forward_input: Union[ForwardInput.Train, ForwardInput.Eval]) -> Union[ForwardOutput.Train, ForwardOutput.Eval]:\n",
        "        # freeze batch normalization modules for each forwarding process just in case model was switched to `train` at any time\n",
        "        for bn_module in self._bn_modules:\n",
        "            bn_module.eval()\n",
        "            for parameter in bn_module.parameters():\n",
        "                parameter.requires_grad = False\n",
        "\n",
        "        image = forward_input.image.unsqueeze(dim=0)\n",
        "        image_height, image_width = image.shape[2], image.shape[3]\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        c1 = self.conv1(image)\n",
        "        c2 = self.conv2(c1)\n",
        "        c3 = self.conv3(c2)\n",
        "        c4 = self.conv4(c3)\n",
        "        c5 = self.conv5(c4)\n",
        "\n",
        "        # Top-down pathway and lateral connections\n",
        "        p5 = self.lateral_c5(c5)\n",
        "        p4 = self.lateral_c4(c4) + F.interpolate(input=p5, size=(c4.shape[2], c4.shape[3]), mode='nearest')\n",
        "        p3 = self.lateral_c3(c3) + F.interpolate(input=p4, size=(c3.shape[2], c3.shape[3]), mode='nearest')\n",
        "        p2 = self.lateral_c2(c2) + F.interpolate(input=p3, size=(c2.shape[2], c2.shape[3]), mode='nearest')\n",
        "\n",
        "        # Reduce the aliasing effect\n",
        "        p4 = self.dealiasing_p4(p4)\n",
        "        p3 = self.dealiasing_p3(p3)\n",
        "        p2 = self.dealiasing_p2(p2)\n",
        "\n",
        "        p6 = F.max_pool2d(input=p5, kernel_size=1, stride=2)\n",
        "\n",
        "        return p4, p3, p2, p6\n"
      ],
      "metadata": {
        "id": "dWc9vzTVESHf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "\n",
        "# Create an instance of the Model\n",
        "model = Model(backbone=ResNet50(pretrained=True), num_classes=10)\n",
        "\n",
        "# Load an image for testing\n",
        "image = Image.open('/content/c98f79f71.png')\n",
        "image = ToTensor()(image)  # Convert the image to a tensor\n",
        "\n",
        "# Create a ForwardInput instance for testing\n",
        "forward_input = Model.ForwardInput.Eval(image=image)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform the forward pass\n",
        "output = model(forward_input)\n",
        "\n",
        "# Print the shapes of the intermediate feature maps\n",
        "p4, p3, p2, p6 = output\n",
        "print(\"p4 shape:\", p4.shape)\n",
        "print(\"p3 shape:\", p3.shape)\n",
        "print(\"p2 shape:\", p2.shape)\n",
        "print(\"p6 shape:\", p6.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu6HU4RHHcRq",
        "outputId": "7cbded5f-ab53-4f74-982e-6c155368f256"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p4 shape: torch.Size([1, 256, 32, 32])\n",
            "p3 shape: torch.Size([1, 256, 64, 64])\n",
            "p2 shape: torch.Size([1, 256, 128, 128])\n",
            "p6 shape: torch.Size([1, 256, 8, 8])\n"
          ]
        }
      ]
    }
  ]
}